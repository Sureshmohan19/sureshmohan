<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Inside PyTorch: How Tensors are Created and Stored</title>
    <style>
        @import url('https://unpkg.com/normalize.css');
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap');

        * {
            box-sizing: border-box;
        }

        :root {
            color-scheme: light dark;
        }

        body {
            background: light-dark(#fff, #000);
            min-height: 100vh;
            display: flex;
            align-items: flex-start;
            justify-content: center;
            padding: 4rem 2rem;
            margin: 0;
            font-family: 'Inter', system-ui;
        }

        body::before {
            --size: 45px;
            --line: color-mix(in hsl, canvasText, transparent 70%);
            content: '';
            height: 100vh;
            width: 100vw;
            position: fixed;
            background: 
                linear-gradient(90deg, var(--line) 1px, transparent 1px var(--size)) 50% 50% / var(--size) var(--size),
                linear-gradient(var(--line) 1px, transparent 1px var(--size)) 50% 50% / var(--size) var(--size);
            mask: linear-gradient(-20deg, transparent 50%, white);
            top: 0;
            pointer-events: none;
            z-index: -1;
        }

        .back-link {
            position: fixed;
            top: 2rem;
            left: 2rem;
            color: brown;
            text-decoration: none;
            opacity: 0.9;
            transition: opacity 0.2s;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .back-link:hover {
            opacity: 1;
            text-decoration: underline;
        }

        article {
            max-width: 85ch;
            width: 100%;
        }

        .blog-header {
            margin-bottom: 3rem;
        }

        h1 {
            font-size: clamp(1.75rem, 4vw, 2.5rem);
            margin: 0 0 0.9rem 0;
            line-height: 1.2;
            font-weight: 600;
        }

        p {
            line-height: 1.6;
            margin: 0 0 1rem 0;
            font-size: 1rem;
            opacity: 0.85;
            font-weight: 400;
        }

        .header-row {
            display: flex;
            gap: 1rem;  
            align-items: center;
            color: brown;
            opacity: 0.90;
            text-decoration: none;
            margin: 1rem 0 0 0;
        }

        .post-divider {
            border: none;
            height: 1px;
            background: color-mix(in hsl, canvasText, transparent 50%);
            margin: 1rem 0 ;
            opacity: 0.6;
        }

        .blog-content p {
            line-height: 1.6;
            margin: 0 0 1.0rem 0;
            font-size: 1rem;
            opacity: 0.85;
            font-weight: 400;
        }

        .blog-content h2 {
            font-size: 1.5rem;
            margin: 3rem 0 1rem 0;
            font-weight: 600;
        }

        .blog-content h4 {
            font-size: 1.15rem;
            margin: 2rem 0 0.5rem 0;
            font-weight: 600;
            font-family: 'Inter', system-ui;
            color: green;
            opacity: 0.85;
            border-left: 3px solid green;
            padding-left: 0.75rem;
        }

        .blog-content code {
            color: brown;
            border: 1px solid rgba(128, 128, 128, 0.4);
            opacity: 0.9;
            padding: 0.5rem;
            border-radius: 6px;
            font-family: 'Inter', system-ui;
            font-size: 0.95em;
            line-height: 1.6;
            display: inline-block;
        }

        .blog-content pre code {
            display: block;  /* ← ADD THIS */
            border: none;     /* Remove duplicate border */
            padding: 0;       /* Remove duplicate padding (pre already has padding) */
        }

        .blog-content > code {
            display: block;
            margin: 1.5rem 0;
        }

        .blog-content pre {
            position: relative;
            padding: 1rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1.5rem 0;
            font-family: 'Inter', system-ui;
            font-size: 0.875rem;
            line-height: 1.5;
            border: 1px solid rgba(139, 115, 85, 0.35);
            color: rgba(0, 0, 0, 0.65);
        }

        .blog-content pre::before {
            content: '';
            position: absolute;
            inset: 0;
            background: hsl(40, 30%, 95%);
            border-radius: inherit;
            z-index: -1;
        }

        .blog-figure {
            margin: 2.5rem 0;
            text-align: center;
        }

        .blog-image {
            display: block;
            max-width: 100%;
            margin: 0 auto;
            padding: 0.75rem;
            background: hsl(40, 30%, 96%);
            border: 1px solid rgba(139, 115, 85, 0.25);
            border-radius: 10px;
        }

        .blog-figure figcaption {
            margin-top: 0.6rem;
            font-size: 0.9rem;
            opacity: 0.7;
        }

        .blog-content ul,
        .blog-content ol {
            font-size: 1rem;
            opacity: 0.85;
            line-height: 1.6;
            margin: 1rem 0 1.5rem 0;
            padding-left: 2rem;
        }

        .blog-content ul {
            list-style-type: disc;
        }

        .blog-content ol {
            list-style-type: decimal;
        }

        .blog-content li {
            margin: 0.5rem 0;
        }

        .blog-content li ul,
        .blog-content li ol {
            margin: 0.5rem 0;
        }

        .blog-content li ul {
            list-style-type: circle;
        }

        .blog-content strong,
        .blog-content b {
            font-weight: 600;
            opacity: 0.9;
        }

        .blog-content em,
        .blog-content i {
            font-style: italic;
            opacity: 0.85;
        }

        @media (max-width: 768px) {
            body {
                padding: 2rem 1rem;
            }

            .back-link {
                top: 1rem;
                left: 1rem;
            }

            .blog-content ul,
            .blog-content ol {
                padding-left: 1.5rem;
            }
        }
    </style>
</head>
<body>
    <a href="#" class="back-link" onclick="history.back(); return false;">
        back
    </a>

    <article>
        <header class="blog-header">
            <h1>Inside PyTorch: How Tensors are Created and Stored</h1>
            <p>From dtype and device selection, through allocators and storage, into TensorImpl, 
                    and finally the user-facing tensor, we will walk through the complete lifecycle of a PyTorch tensor.</p>
            <div class="header-row">
                <div class="name">suresh neethimohan</div>
                <div class="date">29-12-2025</div>
            </div>
        </header>

        <hr class="post-divider">

        <div class="blog-content">
            <p>
                I have long made use of PyTorch, yet until now I had never truly paused to ask what lies beneath this vast and weighty library of tensors. 
                I know I am not alone in this; indeed, most who use it walk the same road. 
                And so, I resolved to turn back, to look beneath the surface, and to write down, step by step, what I come to understand of PyTorch’s inner workings.
            </p>

            <p>
                Let us begin our walk with <em>torch.empty</em>, the first stop in this series on PyTorch. 
                Here we shall look closely at how tensors come into being, and how they are held in memory. 
                PyTorch offers many kinds of tensors — empty, ones, zeros, and more — but to chase them all at once would only cloud the mind. 
                For the sake of clarity, I cast a simple coin and chose one path to follow. 
                That path, for now, is <em>torch.empty</em>.
            </p>

            <code>
                | All code references in this post correspond to PyTorch commit 17a2688dd52 on the main branch.
            </code>

            <p>
                Before we set foot upon this path, there is one matter of great importance that must be understood. 
                If you clone the repository and search in earnest for the function <em>torch.empty()</em>, you will find nothing at all. 
                Truly, it is nowhere to be seen. This is not an oversight. 
                PyTorch does not write this function plainly in Python within the repository; instead, it is brought into being during the build itself. 
                Only after the code is built, and you search once more, will the function reveal itself.
            </p>

            <p>
                The reason for this choice likely lies in efficiency, and in PyTorch’s careful and elaborate process of code generation, known as <em>torchgen</em>. 
                To explain this machinery in full, and to trace the exact path from definition to the final Python function, would demand an article of its own. 
                For now, consider this a gentle warning to the reader. I shall offer only a small glimpse of this process below, before we turn our attention to the C++ implementation itself.
            </p>

            <h2>1. The Declaration: native_functions.yaml</h2>
            <p>
                We have already said that the Python functions we use each day are not written by hand, but are instead formed during the build process. 
                Yet this raises a fair question: such functions must first be declared somewhere. 
                They cannot arise from nothing. That place is a file named <strong style="color: brown;">native_functions.yaml</strong>
            </p>

            <p>
                This file is vast, almost daunting at first sight, stretching to a little over sixteen thousand lines. 
                It lives at <em>pytorch/aten/src/ATen/native/native_functions.yaml</em>, and within it lies the written record of many operations that PyTorch knows how to perform.
            </p>

            <p>
                If you search this file for the word <em>empty</em>, you will find more than one match. 
                This is curious at first, and perhaps confusing. Which of these, then, is the one we seek? 
                The answer is the function named <em>empty.memory_format()</em>.
            </p>

            <figure class="blog-figure">
                <img
                    src="https://r2-image-worker.mohansuresh333.workers.dev/src/images/torch_empty_doc.png"
                    alt="torch.empty documentation showing the public function signature"
                    class="blog-image"
                />
                <figcaption>
                    The public <em>torch.empty</em> documentation.  
                </figcaption>
            </figure>

            <p>
                The documentation makes this clear, and as we shall see below, the definition in <em>native_functions.yaml</em> matches it exactly.
            </p>

            <p>
                The other functions that bear the name <em>empty</em> serve different purposes: 
                some exist for named tensors, others for manual strides, and still others for cases where memory has already been set aside in advance. 
                These paths are important in their own right, but they are not the path we shall walk today. 
                For now, we set them aside and keep our attention on a single door.
            </p>


            <h4>empty.memory_format()</h4>
            <pre><code>
- func: empty.memory_format(
    SymInt[] size,
    *,
    ScalarType? dtype=None,
    Layout? layout=None,
    Device? device=None,
    bool? pin_memory=None,
    MemoryFormat? memory_format=None
  ) -&gt; Tensor
  dispatch:
    CPU: empty_cpu
    CUDA: empty_cuda
    MPS: empty_mps
    Meta: empty_meta_symint
    MkldnnCPU: empty_mkldnn
    SparseCPU, SparseCUDA, SparseMPS: empty_sparse
    SparseMeta: empty_sparse_symint
    SparseCsrCPU, SparseCsrCUDA: empty_sparse_compressed
    SparseCsrMeta: empty_sparse_compressed_symint
            </code></pre>
            <p>
                Let us now look more closely at how <em>empty.memory_format()</em> is written in this file. 
                At first glance, the definition may seem dense, but it follows a clear and deliberate structure.
            </p>

            <p>
                The <strong style="color: brown;">function name</strong> itself follows a simple pattern: <em>&lt;name&gt;.&lt;variant&gt;</em>. 
                Here, <em>empty</em> is the base operation, while <em>memory_format</em> marks a specific variant. 
                PyTorch often defines several such variants for a single operation, such as <em>empty.names</em> or <em>empty.out</em>. 
                This system allows different interfaces to exist side by side, while still sharing the same underlying ideas and, in many cases, the same implementation.
            </p>

            <p>
                The <strong style="color: brown;">parameters</strong> of this function are divided with care. 
                The first argument, <em>SymInt[] size</em>, describes the shape of the tensor. 
                These are symbolic integers, which means the size may be known only at runtime, as is common in compiled or traced code paths. 
                After this comes the asterisk, a quiet but important marker, telling us that everything that follows must be passed by keyword.
            </p>

            <p>
                These <strong style="color: brown;">keyword-only</strong> arguments control the finer details of tensor creation: the scalar type, the layout, the target device, whether memory should be pinned, and finally the desired memory format.
                Each of these is optional, allowing PyTorch to choose sensible defaults when the user does not specify them.
            </p>

            <p>
                The <strong style="color: brown;">return type</strong> is simple. This function yields a single tensor. 
                Some operations in PyTorch return multiple values, but <em>empty.memory_format()</em> does not. 
                It exists for one purpose only: to produce a tensor of the requested shape and properties.
            </p>

            <p>
                Below the function signature lies the <strong style="color: brown;">dispatch table</strong>, and it is here that the real work begins. 
                This table maps different execution contexts to their corresponding C++ implementations. 
                When you call <em>torch.empty</em> on the CPU, on a CUDA device, or on Apple’s MPS backend, the dispatcher examines the arguments, selects the appropriate dispatch key, and forwards the call to the correct kernel.
            </p>

            <p>
                In this way, PyTorch presents a single, calm Python interface, while hiding a great deal of complexity beneath it. 
                The same call can create a tensor on the CPU, on a GPU, or even on a meta device used only for shape inference, all without the user needing to change their code.
            </p>

            <p>
                There are other <em>empty</em> variants defined in this file, and the curious reader may wish to glance at them.
                We will not follow those paths here, as they would lead us away from the story we are telling.
            </p>

            <p>
                For now, it is enough to note one simple fact. 
                From the dispatch table, we can see that the CPU path leads to a function named <strong style="color: brown;">empty_cpu</strong>
                That is where we shall go next. One might ask why we do not begin with CUDA, since it is so central to modern deep learning. 
                The answer is simple. We are walking, not running. And so, as with many things, we begin with the CPU.
            </p>

            <figure class="blog-figure">
                <img
                    src="https://r2-image-worker.mohansuresh333.workers.dev/src/images/inside_pytorch-1.png"
                    alt="How torch.empty is dispatched from the Python API to backend implementations"
                    class="blog-image"
                />
                <figcaption>
                    The path taken by <em>torch.empty</em>, from its Python entry point to backend-specific kernels.
                </figcaption>
            </figure>

            <h2>2. The CPU Path: Following empty_cpu</h2>

            <p>
                Before we step into the implementation of empty_cpu, there is a small question to answer: where is it actually defined?
            </p>

            <p>
                If you clone the PyTorch repository and simply search for <strong>empty_cpu</strong> , you will quickly be flooded with results. Declarations in headers, references in generated code, and entries in configuration files all appear at once. At first glance, it is not obvious which of these contains the real backend logic.
            </p>

            <p>
                The key to resolving this lies in namespaces. In <strong>native_functions.yaml</strong>, we saw that the CPU dispatch points to a symbol named <em>empty_cpu</em>. But in C++, such names rarely live in the global scope.
            </p>

            <p>
                Although we are setting aside the deeper details of PyTorch’s code generation machinery for now, there is one rule that matters here. The default kernel namespace is <em>at::native</em>. This is not a guess; it is written directly into the code that drives generation.
            </p>

             <code>
            torchgen/model.py<br>
            <strong style="color: green;">60</strong>: DEFAULT_KERNEL_NAMESPACE = "at::native"
            </code>

            <p>
                With this in mind, the name <strong>empty_cpu</strong> from the YAML file expands naturally into <em>at::native::empty_cpu</em>. Once we search within that namespace, the noise falls away.
            </p>
           
            <p>
                Following this trail leads us straight to <em>aten/src/ATen/native/TensorFactories.cpp</em>. There, wrapped inside the expected namespace, we find the function we have been seeking.
            </p>

            <p>
                Now that we know where empty_cpu lives, we can finally turn our attention to what it does. From here on, the discussion becomes concrete, as we begin to see how PyTorch allocates memory on the CPU.
            </p>

            <h4>at::native::empty_cpu in TensorProperties.cpp</h4>
            
            <p> The full source is shown below for the curious reader who wishes to look a little closer.</p>

            <pre><code>
Tensor empty_cpu(
    IntArrayRef size,
    std::optional&lt;ScalarType&gt; dtype_opt,
    std::optional&lt;Layout&gt; layout_opt,
    std::optional&lt;Device&gt; device_opt,
    std::optional&lt;bool&gt; pin_memory_opt,
    std::optional&lt;c10::MemoryFormat&gt; memory_format_opt) {
  Tensor result = at::detail::empty_cpu(
      size,
      dtype_opt,
      layout_opt,
      device_opt,
      pin_memory_opt,
      memory_format_opt);
  // See Note [Enabling Deterministic Operations]
  if (C10_UNLIKELY(
          at::globalContext().deterministicAlgorithms() &&
          at::globalContext().deterministicFillUninitializedMemory())) {
    fill_empty_deterministic_(result);
  }
  return result;
}
</code></pre>

            <p>At first sight, this function mirrors what we saw earlier in <em>native_functions.yaml</em>. 
            The parameters line up one for one: the tensor size, followed by a sequence of optional arguments for data type, layout, device, pinned memory, and memory format. 
            In C++, these optional values are represented using <em>std::optional</em>, reflecting the same flexibility exposed at the Python level.</p>

            <p>The return type is a single <em>Tensor</em>, just as the YAML definition promised. 
            There is no extra logic here to construct or modify the tensor directly. Instead, this function serves as a thin wrapper.</p>

            <p>The actual allocation work is delegated to <em>at::detail::empty_cpu</em>, which lives one layer deeper in the implementation. 
            There is also a small conditional block related to deterministic behavior. We will set that aside for now and return to it later, once the main allocation path is clear.</p>

            <p>With that, our path forward is obvious. To understand how memory is truly allocated on the CPU, we must now step into the <em>detail</em> namespace and examine <em>empty_cpu</em> there.</p>

            <h4>at::detail::empty_cpu in EmptyTensor.cpp(primary)</h4>

            <p>If you now search for <em>empty_cpu</em> inside the <em>detail</em> namespace, you will find more than one definition. 
            In fact, in <em>aten/src/ATen/EmptyTensor.cpp</em>, there are three functions that share this name.</p>
            
            <pre><code>
TensorBase empty_cpu(
    IntArrayRef size,
    std::optional&lt;ScalarType&gt; dtype_opt,
    std::optional&lt;Layout&gt; layout_opt,
    std::optional&lt;Device&gt; device_opt,
    std::optional&lt;bool&gt; pin_memory_opt,
    std::optional&lt;c10::MemoryFormat&gt; memory_format_opt) {
  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(device_or_default(device_opt).type() == DeviceType::CPU);
  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(layout_or_default(layout_opt) == Layout::Strided);

  auto pin_memory = pinned_memory_or_default(pin_memory_opt);
  auto dtype = dtype_or_default(dtype_opt);
  return empty_cpu(size, dtype, pin_memory, memory_format_opt);
}
</code></pre>
            
            <p>So which one is ours? The answer lies, once again, in the function signature. 
            The version shown above accepts the same set of parameters that the <em>at::native::empty_cpu</em> wrapper passes along. 
            The other overloads take different arguments and therefore cannot be the target of this call.</p>

            <p>With that match established, the structure of this function becomes clear. 
            The <em>size</em> argument is required, just as it was before, while the remaining parameters are optional. 
            Two debug-only assertions follow. These checks confirm that the device is indeed the CPU, and that the layout is strided. 
            Both conditions are expected here, but the assertions serve as a safeguard during development.</p>

            <p>After this brief validation, the function sheds the optional wrappers, resolves defaults where needed, and forwards the call once more. 
            Only the essential pieces remain: the size, the data type, the pinned-memory flag, and the memory format.</p>

            <p>And so the path continues. To see where memory is actually reserved, we must now follow this final call to the next <em>empty_cpu</em> below.</p>

            <h4>at::detail::empty_cpu in EmptyTensor.cpp(secondary)</h4>
            <p>At this point, the layers of indirection begin to fall away. 
            The function signature is simpler now. All optional arguments have been resolved, and only the essentials remain: the tensor size, the scalar type, whether memory should be pinned, and the requested memory format.</p>

            <pre><code>
TensorBase empty_cpu(
    IntArrayRef size,
    ScalarType dtype,
    bool pin_memory,
    std::optional&lt;c10::MemoryFormat&gt; memory_format_opt) {
  auto allocator = GetCPUAllocatorMaybePinned(pin_memory);
  constexpr c10::DispatchKeySet cpu_ks(c10::DispatchKey::CPU);
  return empty_generic(size, allocator, cpu_ks, dtype, memory_format_opt);
}
</code></pre>

            <p>
            The first line selects an allocator. The choice depends on the <em>pin_memory</em> flag. 
            If pinned memory is requested, a specialized allocator is used; otherwise, the standard CPU allocator is chosen. 
            We will return to the allocator itself shortly.
            </p>

            <p>
            Next, a dispatch key set is constructed. Here it contains only a single key, <em>CPU</em>. 
            This value records the backend context in which the tensor will live and will travel with the tensor as it moves through the system.
            </p>

            <p>
            With these pieces in hand, the function delegates once more. 
            The actual work is passed to <em>empty_generic</em>, along with the size, allocator, dispatch key set, data type, and memory format.
            </p>

            <p>
            Rather than detouring into the side details just yet, we will follow the main path. 
            To understand how a tensor is truly created, we must first step into <em>empty_generic</em>. 
            Only after that will it make sense to return and examine how the allocator and dispatch keys shape the result.
            </p>

            <h4>at::detail::empty_generic in EmptyTensor.cpp</h4>
            
            <p>If you search for <em>empty_generic</em> in the codebase, you will find two definitions. 
            One is intended for symbolic shapes, used when sizes are not concrete at runtime. That path is not our concern here.</p>

            <pre><code>
TensorBase empty_generic(
    IntArrayRef size,
    c10::Allocator* allocator,
    c10::DispatchKeySet ks,
    ScalarType scalar_type,
    std::optional&lt;c10::MemoryFormat&gt; memory_format_opt) {
  return _empty_generic(size, allocator, ks, scalar_type, memory_format_opt);
}
</code></pre>

            <p>The version shown above is the one reached from <em>empty_cpu</em>. 
            Its role is modest and precise. It accepts exactly the arguments prepared in the previous step: the size, allocator, dispatch key set, scalar type, and memory format.</p>

            <p>There is no additional logic here. This function exists mainly as a thin wrapper, forwarding everything directly to <em>_empty_generic</em>.</p>

            <p>And so the trail continues. To see where memory is finally allocated and a tensor is truly brought into being, we must now step into <em>_empty_generic</em> itself.</p>

            <h4>at::detail::_empty_generic in EmptyTensor.cpp</h4>

            <p>This is the point at which all the earlier preparation finally comes together. 
            Here, the tensor is no longer an abstract idea passed along through layers of wrappers. 
            Memory is sized, storage is created, and a concrete tensor object begins to take shape.</p>

            <pre><code>
static TensorBase _empty_generic(
    ArrayRef&lt;T&gt; size,
    c10::Allocator* allocator,
    c10::DispatchKeySet ks,
    ScalarType scalar_type,
    std::optional&lt;c10::MemoryFormat&gt; memory_format_opt) {
  at::detail::check_size_nonnegative(size);
  at::detail::raise_warning_for_complex_half(scalar_type);
  caffe2::TypeMeta dtype = scalarTypeToTypeMeta(scalar_type);
  auto size_bytes = computeStorageNbytesContiguous(size, dtype.itemsize());

  auto storage_impl = c10::make_intrusive&lt;StorageImpl&gt;(
      c10::StorageImpl::use_byte_size_t(),
      size_bytes,
      allocator,
      /*resizeable=*/true);

  auto tensor = detail::make_tensor_base&lt;TensorImpl&gt;(
      std::move(storage_impl), ks, dtype);

  // Default TensorImpl has size [0]
  if (ks.has(c10::DispatchKey::Meta) || size.size() != 1 || size[0] != 0) {
    tensor.unsafeGetTensorImpl()
        ->generic_set_sizes_contiguous(size);
  }

  if (memory_format_opt.has_value()) {
    if (*memory_format_opt != MemoryFormat::Contiguous) {
      tensor.unsafeGetTensorImpl()
          ->empty_tensor_restride(*memory_format_opt);
    }
  }

  return tensor;
}
</code></pre>

            <p>
            The function starts with a few quiet checks. It ensures that the requested size is valid and raises a warning for certain edge cases in the scalar type. 
            The scalar type is then translated into a lower-level representation, from which the total number of bytes required for storage is computed.
            </p>

            <p>
            With this information, a <em>StorageImpl</em> is created using the allocator chosen earlier. 
            This is the moment where memory is actually reserved. 
            The storage is then wrapped inside a <em>TensorImpl</em>, along with the dispatch key set and type information, forming the core of the tensor.
            </p>

            <p>
            The tensor’s size and memory layout are applied next. 
            If a non-default memory format is requested, the tensor is restrided accordingly. 
            Once this is done, the tensor is complete and ready to be returned.
            </p>

            <p>
            This marks the end of the high-level factory path. 
            From here onward, the story moves into the <em>c10</em> backend, where concepts like <em>TensorImpl</em>, <em>Storage</em>, allocators, and dispatch keys live in full detail.
            </p>

            <p>
            We have followed <em>torch.empty</em> as far as it can take us without crossing that boundary. 
            Now, if we wish to truly understand how tensors exist in PyTorch, it is time to step into the depths of the <em>c10</em> layer.
            </p>

            <code>| This is also a good moment to pause, for what follows grows deeper and more interesting still.</code>

            <h2>3. c10 - The Backend</h2>

            <p>We start with the allocator, because this is the first concrete decision the system makes once we cross into the c10 boundary. 
                In <strong style="color: brown;">empty_cpu</strong>, everything that follows is downstream of how and where memory is obtained.</p>

            <h4>GetCPUAllocatorMaybePinned function</h4>

            <pre><code>
c10::Allocator* GetCPUAllocatorMaybePinned(bool pin_memory) {
  if (pin_memory) {
    // comments are omitted for clarity

    std::optional&lt;c10::DeviceType&gt; opt_device_type = std::nullopt;

    if (at::globalContext().hasCUDA()) {
      opt_device_type = c10::DeviceType::CUDA;
    } else {
      opt_device_type = at::getAccelerator(false);
    }
    if (opt_device_type.has_value()) {
      return at::globalContext().getPinnedMemoryAllocator(opt_device_type);
    } else {
      TORCH_CHECK(
          false,
          "pin_memory=True requires a CUDA or other accelerator backend; "
          "no pinned memory allocator is available on this system.")
    }
  }

  return c10::GetCPUAllocator();
}
</code></pre>

            <p>This function answers a single, practical question: which allocator should be used for a CPU tensor?</p>
            <p>The decision hinges on the <em>pin_memory</em> flag. If pinned memory is not requested, the answer is simple. The function returns the default CPU allocator, and the story ends there.</p>
            <p>When <em>pin_memory</em> is set to true, the path becomes more careful. Pinned memory only makes sense when an accelerator is present, since its purpose is to enable faster transfers between CPU memory and devices such as GPUs. The function therefore checks the global context to see which accelerator backends are available.</p>
            <p>CUDA is given priority if it is present. If not, PyTorch looks for another registered accelerator. If an accelerator is found, the corresponding pinned-memory allocator is returned. If none exists, the function fails loudly, explaining that pinned memory cannot be provided on this system.</p>

            <p>At this stage, nothing more than a decision has been made. 
                This function does not allocate memory, nor does it touch any tensor data. It merely selects the allocator through which all future allocation will flow.</p>

            <p>With that choice settled, the path finally reaches the point where memory is requested. The next line is where storage is created and ownership begins.</p>

            <h4>Dispatch Keys</h4>
            <p>Alongside the allocator, a second piece of information is fixed here: the dispatch key.</p>

            <p>The line <em>DispatchKey::CPU</em> simply records that this tensor belongs to the CPU backend. 
            At this stage, there is nothing subtle or dynamic about it. No kernels are chosen and no dispatch occurs. 
            The key is stored as part of the tensor’s identity and will be consulted later, when operations are applied.</p>

            <p>The mechanics behind dispatch keys are relatively straightforward, though implemented with care. 
            Curious readers may glance at <strong style="color: brown;">c10/core/DispatchKey.h</strong> to see how these keys are defined and combined. 
            We will return to this topic in greater detail later in the series, once the rest of the tensor machinery is firmly in place.</p>

            <h4>intrusive_ptr</h4>
            <pre><code>
auto storage_impl = c10::make_intrusive&lt;StorageImpl&gt;(
    c10::StorageImpl::use_byte_size_t(),
    size_bytes,
    allocator,
    /*resizeable=*/true);
</code></pre>

            <p>Having fixed both the allocator and the dispatch key, the next inevitable step is to examine the StorageImpl created inside _empty_generic. 
                This single line conceals a short but important chain of calls that culminates in a heap allocation.</p>

            <pre><code>
inline intrusive_ptr&lt;TTarget, NullType&gt; make_intrusive(Args&amp;&amp;... args) {
  return intrusive_ptr&lt;TTarget, NullType&gt;::make(std::forward&lt;Args&gt;(args)...);
}
</code></pre>

            <p>The call does not directly construct a StorageImpl. 
                Instead, it enters PyTorch's intrusive reference-counting machinery, implemented in c10/util/intrusive_ptr.h. 
                The make_intrusive function exists solely to forward arguments and infer types — here, TTarget becomes StorageImpl, and the constructor arguments pass through unchanged.</strong>.</p>

            <p>Inside intrusive_ptr::make, the actual allocation occurs:</p>
            <pre><code>
static intrusive_ptr make(Args&amp;&amp;... args) {
  return intrusive_ptr(new TTarget(std::forward&lt;Args&gt;(args)...));
}
</code></pre>

            <p>This is the moment the C++ object comes into existence. The new operator allocates memory on the heap and invokes the StorageImpl constructor with the forwarded arguments. The resulting raw pointer is immediately wrapped inside an intrusive_ptr.</p>
            <p>Unlike std::shared_ptr, which stores its reference count in a separate control block, intrusive_ptr embeds the count within the object itself. This design allows PyTorch to tightly couple object lifetime with reference counting, which proves essential for Python interoperability.</p>
            <p>The remainder of intrusive_ptr.h handles reference counting mechanics, weak pointers, and threading concerns. These details matter deeply for correctness, but they are not required to understand how storage is first created.</p>
            <p>At this point, memory ownership has begun. A StorageImpl object exists on the heap, bound to a chosen allocator and managed by intrusive reference counting.</p>
            <p>Next, we examine StorageImpl itself: what it stores, what it owns, and how tensors are built on top of it.</p>
           
            <h4>StorageImpl</h4>

            <p>
If you search for the constructors of <em>StorageImpl</em> in
<strong style="color: brown;">c10/core/StorageImpl.h</strong>, you will find two of them.
A closer look reveals that the following constructor is the one reached from our call to <em>make_intrusive</em>.
</p>

<pre><code>
StorageImpl(
    use_byte_size_t /*use_byte_size*/,
    const SymInt& size_bytes,
    at::Allocator* allocator,
    bool resizable)
    : StorageImpl(
          use_byte_size_t(),
          size_bytes,
          size_bytes.is_heap_allocated()
              ? allocator->allocate(0)
              : allocator->allocate(size_bytes.as_int_unchecked()),
          allocator,
          resizable) {}
</code></pre>

            <p>This constructor accepts exactly what was passed down from above: a tag indicating byte-based sizing, the requested size in bytes, the allocator, and a flag marking whether the storage may be resized.</p>
            <p>Its first decision concerns the size itself. If <em>size_bytes</em> is heap-allocated, the allocator is called with zero.
                Otherwise, the symbolic size is resolved into a concrete integer, and that value is used for allocation.</p>
            <p>And the allocator invoked here is no stranger to us. It is the same CPU allocator selected earlier, now finally asked to provide memory.
                This is the moment at which real storage is requested from the system.</p>
            <p>Once allocation completes, this constructor immediately forwards the call to the primary constructor, passing along the newly created <em>DataPtr</em>.
                That constructor performs no further allocation. It simply records the pointer, the size, and a handful of internal properties.</p>
            <p>The chain is now complete. The call to <em>make_intrusive</em> led to <em>new StorageImpl</em>, which reached this constructor, which in turn invoked <em>allocator-&gt;allocate()</em>.
                To understand what truly happens next, we must now step into the CPU allocator itself.</p>

            <h4>CPUAllocator::allocate</h4>

            <p>Before we reach the operating system itself, there is one final layer to pass through.
            When <em>StorageImpl</em> calls <em>allocator-&gt;allocate(size_bytes)</em>, it is invoking a virtual method defined on the base <em>Allocator</em> interface.
            Which implementation actually runs depends on the allocator we selected earlier.</p>

            <p>In our case, that allocator is the default CPU allocator.
            Concretely, this is an instance of <strong style="color: brown;">DefaultCPUAllocator</strong>, defined in
            <em>c10/core/CPUAllocator.cpp</em>. Let us see what happens when its <em>allocate</em> method is invoked.</p>

<pre><code>
struct C10_API DefaultCPUAllocator final : at::Allocator {
  DefaultCPUAllocator() = default;
  
  at::DataPtr allocate(size_t nbytes) override {
    void* data = nullptr;
    try {
      data = c10::alloc_cpu(nbytes);
    } catch (c10::Error& e) {
      profiledCPUMemoryReporter().OutOfMemory(nbytes);
      throw e;
    }
    profiledCPUMemoryReporter().New(data, nbytes);
    return {data, data, &ReportAndDelete, at::Device(at::DeviceType::CPU)};
  }
</code></pre>

            <p>This function is small, but it marks the boundary between PyTorch’s abstractions and the system allocator.
                It begins by declaring a raw pointer and immediately calls <em>c10::alloc_cpu(nbytes)</em>.</p>

            <p>If the allocation fails, the error is propagated upward.
                Profiling hooks are enabled here to track allocations and failures for debugging and diagnostics, but we will not dwell on them.</p>

            <p>Assuming the allocation succeeds, the pointer is wrapped and returned. We will postpone a detailed discussion of what this wrapper contains and why it exists.
                That will be our next stop.</p>

            <p>For now, let us follow the call further down, into <em>c10::alloc_cpu</em>, where memory is finally requested from the operating system.</p>

            <h4>alloc_cpu</h4>

            <p>Up to this point, everything we have seen lives entirely within PyTorch. Now we arrive at the edge.
            The function <strong style="color: brown;">c10::alloc_cpu</strong>, defined in <em>c10/core/impl/alloc_cpu.cpp</em>, is where PyTorch makes the actual system call.</p>

<pre><code>
void* alloc_cpu(size_t nbytes) {
  if (nbytes == 0) {
    return nullptr;
  }
  
  CAFFE_ENFORCE(
      ((ptrdiff_t)nbytes) >= 0,
      "alloc_cpu() seems to have been called with negative number: ",
      nbytes);

  void* data = nullptr;
</code></pre>

            <p>The function begins with two simple guards. If zero bytes are requested, (for empty tensors with zero elements), no memory is needed, so <em>alloc_cpu</em> simply returns <em>nullptr</em> while the tensor still carries its shape and type metadata.
            No memory needs to be allocated. The second check ensures the size is non-negative, catching invalid requests early.</p>

<pre><code>
int err = posix_memalign(&data, c10_compute_alignment(nbytes), nbytes);
CAFFE_ENFORCE(
    err == 0,
    "DefaultCPUAllocator: can't allocate memory: you tried to allocate ",
    nbytes,
    " bytes. Error code ",
    err,
    " (",
    c10::utils::str_error(err),
    ")");
</code></pre>

            <p>This is the moment when the operating system is asked for memory.
                On Unix-like systems, PyTorch uses <strong style="color: brown;">posix_memalign</strong> to allocate memory with a specific alignment.
                On Windows, PyTorch uses platform-specific APIs to request aligned memory, ensuring the same alignment guarantees as <em>posix_memalign</em> on Unix systems.</p>

            <p>
            Alignment matters for performance. Many CPU instructions and vectorized operations require data to be aligned to fixed boundaries, commonly 64 bytes.
            The helper <em>c10_compute_alignment</em> selects an appropriate alignment based on the requested size and system configuration.</p>

<pre><code>
if (is_thp_alloc(nbytes)) {
#ifdef __linux__
  madvise(data, nbytes, MADV_HUGEPAGE);
#endif
}
</code></pre>

            <p>For sufficiently large allocations, PyTorch may advise the kernel to back this memory with transparent huge pages.
            This is an optimization hint only. If it cannot be applied, execution continues normally.
            </p>

<pre><code>
NUMAMove(data, nbytes, GetCurrentNUMANode());
</code></pre>

            <p>The allocator is also NUMA-aware.
            On multi-socket systems, memory is placed on the current NUMA node so that it remains physically close to the executing CPU, reducing access latency.</p>

<pre><code>
if (FLAGS_caffe2_cpu_allocator_do_zero_fill) {
  memset(data, 0, nbytes);
} else if (FLAGS_caffe2_cpu_allocator_do_junk_fill) {
  memset_junk(data, nbytes);
}

return data;
}
</code></pre>

            <p>Finally, the memory may be initialized. Depending on configuration, it can be zero-filled or filled with a known pattern to expose bugs that rely on uninitialized data.
                If neither mode is enabled, the memory is returned as-is.</p>

            <p>At this point, the system has granted PyTorch a block of memory.
                This raw pointer now begins its journey back upward, where it will be wrapped, stored, and eventually associated with a tensor.
            </p>

            <p>Before we return to <em>StorageImpl</em>, there is one important structure we have deliberately skipped.
                The allocator does not return a raw pointer directly. Instead, it returns a <em>DataPtr</em>.
                To understand how memory ownership, lifetime, and deletion are handled, we must look at <em>DataPtr</em> next.</p>

            <h4>DataPtr</h4>

            <p>The raw pointer returned from <em>alloc_cpu</em> is not stored directly inside <em>StorageImpl</em>. 
            Before that happens, it is wrapped in a small but important abstraction called <strong style="color: brown;">DataPtr</strong>.</p>

            <p>Back in <em>DefaultCPUAllocator::allocate</em>, once memory is successfully allocated, the allocator returns a <em>DataPtr</em> constructed like this:</p>

<pre><code>
return {data, data, &ReportAndDelete, at::Device(at::DeviceType::CPU)};
</code></pre>

            <p>This invokes one of <em>DataPtr</em>'s constructors. Let us look at its definition. </p>

<pre><code>
class C10_API DataPtr {
private:
  c10::detail::UniqueVoidPtr ptr_;
  Device device_;

public:
  DataPtr(void* data, void* ctx, DeleterFnPtr ctx_deleter, Device device)
      : ptr_(data, ctx, ctx_deleter), device_(device) {}
};
</code></pre>

            <p>A <em>DataPtr</em> stores two things: a <em>UniqueVoidPtr</em> that owns the memory, and a <em>Device</em> that records where this memory lives.</p>

            <p>The <em>data</em> argument is the actual address returned by <em>alloc_cpu</em>. 
                This is the pointer that tensor operations will eventually read from and write to.</p>

            <p>The <em>ctx</em> argument is the context pointer that will be passed to the deleter when the memory is freed. 
                For the CPU allocator, this is simply the same pointer as <em>data</em>. More complex allocators may use a different context object to carry additional metadata.</p>

            <p>The third argument is the deleter function. When the <em>DataPtr</em> is destroyed, this function will be invoked with the context pointer. 
                In our case, it ultimately returns memory back to the system.</p>

            <p>Finally, the device field records that this allocation belongs to the CPU backend.</p>

            <p>Internally, ownership is handled by <em>UniqueVoidPtr</em>. This is where the lifetime rules are enforced.</p>

<pre><code>
class UniqueVoidPtr {
private:
  void* data_;
  std::unique_ptr&lt;void, DeleterFnPtr&gt; ctx_;

public:
  UniqueVoidPtr(void* data, void* ctx, DeleterFnPtr ctx_deleter)
      : data_(data), ctx_(ctx, ctx_deleter ? ctx_deleter : &deleteNothing) {}
};
</code></pre>

            <p>The key idea here is separation. 
                The pointer used for data access does not own the memory. Ownership instead lives with the context pointer managed by a <em>std::unique_ptr</em>.</p>

            <p> This design allows PyTorch to expose one pointer to users while freeing memory using another. 
                Although both pointers are the same for CPU allocations, this flexibility is essential for CUDA, shared memory, and externally managed buffers.</p>

            <p>Once constructed, the <em>DataPtr</em> is returned to the <em>StorageImpl</em> constructor that requested it.
                The returned <em>DataPtr</em> is immediately forwarded to the primary <em>StorageImpl</em> constructor.</p>

<pre><code>
StorageImpl(
    use_byte_size_t /*use_byte_size*/,
    SymInt size_bytes,
    at::DataPtr data_ptr,
    at::Allocator* allocator,
    bool resizable)
    : data_ptr_(std::move(data_ptr)),
      size_bytes_(std::move(size_bytes)),
      size_bytes_is_heap_allocated_(size_bytes_.is_heap_allocated()),
      resizable_(resizable),
      received_cuda_(false),
      allocator_(allocator) {
  if (resizable) {
    TORCH_INTERNAL_ASSERT(
        allocator_, "For resizable storage, allocator must be provided");
  }
  refresh_has_data_ptr_check();
}
</code></pre>

            <p>Here, ownership is transferred into <em>StorageImpl</em> via a move. From this moment on, the storage object controls the lifetime of the memory.</p>
            <p>When <em>StorageImpl</em> is eventually destroyed, its <em>DataPtr</em> is destroyed as well. That destruction triggers the deleter, returning the memory back to the system.</p>
            <p>At this point, the storage layer is complete. 
                We now have a heap-allocated <em>StorageImpl</em>, managed by an <em>intrusive_ptr</em>, holding a <em>DataPtr</em> that knows how to free its memory correctly.</p>

            <p>We now return to <em>_empty_generic</em>. With what we have seen so far, the next step should no longer be a surprise.</p>

            <h4>make_tensor_base</h4>

            <p>With the storage layer complete, we return once more to <em>_empty_generic</em>, where the next step quietly awaits.</p>

<pre><code>
auto tensor = detail::make_tensor_base&lt;TensorImpl&gt;(
    std::move(storage_impl), ks, dtype);
</code></pre>

            <p>This single line marks an important transition. 
                Until now, we have been dealing only with memory: allocators, raw pointers, and storage. Here, that memory is finally lifted into the world of tensors.</p>

            <p>The helper <em>make_tensor_base</em>, defined in <em>aten/src/ATen/core/TensorBase.h</em>, is intentionally small.</p>

<pre><code>
template &lt;typename T, typename... Args&gt;
TensorBase make_tensor_base(Args&amp;&amp;... args) {
  return TensorBase(c10::make_intrusive&lt;T&gt;(std::forward&lt;Args&gt;(args)...));
}
</code></pre>

            <p>All it does is forward its arguments into <em>c10::make_intrusive&lt;TensorImpl&gt;</em>. 
            Just as before with <em>StorageImpl</em>, this expands into a heap allocation:</p>

            <p><em>new TensorImpl(std::move(storage_impl), ks, dtype)</em></p>

            <p>The storage is moved, not copied, and ownership passes cleanly into the newly created <em>TensorImpl</em>. 
                That <em>TensorImpl</em> is then wrapped in an <em>intrusive_ptr</em> and placed inside a <em>TensorBase</em>, which acts as the lightweight handle exposed throughout PyTorch.</p>

            <p>At this point, we stop. We now know <em>where</em> <em>TensorImpl</em> comes from and <em>how</em> it is constructed, but not yet <em>what</em> it does.</p>

            <p> To truly understand how a tensor gains its shape, strides, and behavior, we must step into the <em>TensorImpl</em> constructor itself. That is where the next phase begins.</p>

            <h4>TensorImpl</h4>

            <p>We have seen how <em>make_intrusive</em> allocates a <em>TensorImpl</em> on the heap. 
                Now we turn our attention to what happens inside its constructor, where storage, dispatch keys, and data type finally come together to form the core of a tensor.</p>

            <p>The call from <em>make_tensor_base</em> lands here, in <em>c10/core/TensorImpl.cpp</em>:</p>

<pre><code>
TensorImpl::TensorImpl(
    Storage&& storage,
    DispatchKeySet key_set,
    const caffe2::TypeMeta data_type)
    : TensorImpl(std::move(storage), key_set, data_type, storage.device()) {}
</code></pre>

            <p>This constructor is deliberately minimal. 
                It immediately forwards to a more complete constructor, extracting the device from the storage and passing it along. 
                This ensures that all <em>TensorImpl</em> instances are initialized through a single, consistent path.</p>

            <p>The real work happens here:</p>

<pre><code>
TensorImpl::TensorImpl(
    ImplType /*type*/,
    Storage&& storage,
    DispatchKeySet key_set,
    const caffe2::TypeMeta data_type)
    : storage_(std::move(storage)),
      numel_(0),
      data_type_(data_type),
      device_opt_(storage_.device()),
      key_set_(key_set - c10::python_ks) {
  init_bitfields();
  if (!is_inference()) {
    version_counter_ = VariableVersion(/*version=*/0);
  }
}
</code></pre>

            <p>The constructor begins by moving the storage into the <em>storage_</em> member. 
                Ownership transfers here. From this moment on, the <em>TensorImpl</em> owns the <em>StorageImpl</em>, which in turn owns the underlying memory.</p>

            <p>The <em>numel_</em> field is initialized to zero. 
                Although the tensor does have a size, that information has not yet been applied at this stage. 
                Sizes and strides are assigned immediately after construction, at which point <em>numel_</em> will be computed correctly.</p>

            <p>The <em>data_type_</em> field records the scalar type of the tensor. 
                This value originated earlier in <em>_empty_generic</em> and is now permanently associated with the tensor.</p>

            <p>The <em>device_opt_</em> field is derived directly from the storage. 
                This guarantees that the tensor’s device always matches the device on which its memory was allocated.</p>

            <p>The <em>key_set_</em> stores the dispatch keys we saw earlier when the CPU dispatch key was constructed. 
                Here, one adjustment is made: Python-related keys are removed. At this point, the tensor exists purely on the C++ side. 
                If and when it is exposed to Python, those keys will be added back later.</p>

            <p>Next, <em>init_bitfields()</em> initializes a set of internal flags that describe tensor properties such as contiguity and metadata mutability. 
                These flags start in sensible default states and will be refined as the tensor’s layout is finalized.</p>

            <p>Finally, if the tensor participates in autograd, a version counter is created and initialized to zero. 
                This counter allows PyTorch to track in-place mutations and enforce correctness during gradient computation.</p>

            <p>At this stage, the <em>TensorImpl</em> is structurally complete. It owns storage, knows its data type, device, and dispatch keys, and is ready to describe a tensor.</p>

            <p>What remains is the final step: assigning sizes and strides. 
                That happens immediately after construction, back in <em>_empty_generic</em>. Once that is done, the tensor is fully formed and ready for use.</p>

            
            <h4>Shapes and Strides</h4> 
            <p> With the <em>TensorImpl</em> constructed, we return once more to <em>_empty_generic</em>, where the final pieces are put in place. 
                At this stage, memory has already been allocated and wrapped in storage, but the tensor still does not know its shape or how its elements should be laid out. </p> 
<pre><code> 
if (ks.has(c10::DispatchKey::Meta) || size.size() != 1 || size[0] != 0) { 
    tensor.unsafeGetTensorImpl() 
    ->generic_set_sizes_contiguous(size); 
} 

if (memory_format_opt.has_value()) { 
    if (*memory_format_opt != MemoryFormat::Contiguous) { 
        tensor.unsafeGetTensorImpl() 
        ->empty_tensor_restride(*memory_format_opt); 
    } 
} 
</code></pre>

            <p> This is the point where sizes and strides are attached to the tensor. 
                The call to <em>generic_set_sizes_contiguous</em> records the tensor’s shape and computes a matching contiguous stride layout. 
                Only metadata is updated here. No memory is allocated, resized, or moved. </p> 
            <p> Internally, PyTorch distinguishes between two cases. If the sizes are concrete integers, the fast path is taken and strides are computed directly. 
                If the sizes are symbolic, PyTorch records symbolic size and stride information instead, deferring concrete values until they are known. 
                Both paths ultimately establish the same logical view of the tensor. </p> 
            <p> The second block handles memory format. If a non-contiguous format such as <em>channels_last</em> is requested, the tensor is restrided accordingly. 
                Again, this affects only metadata. The underlying storage remains unchanged. </p> 
            <p> This machinery lives in <em>c10/core/TensorImpl.cpp</em> and quickly branches into symbolic shape handling, stride policies, and layout-specific logic. 
                Each of these deserves a careful and separate treatment. </p> 
            <p>For <em>torch.empty</em>, shapes and strides are attached immediately after allocation, using a contiguous layout by default, with no data initialization and no further memory movement.</p>
            <p> For our purposes, it is enough to understand the boundary: storage provides raw memory, while <em>TensorImpl</em> provides meaning. Shapes and strides complete that meaning, turning an anonymous block of bytes into a usable tensor. </p> 
            <p> We will stop here and not descend further into symbolic shapes and stride internals. 
                That path is deep, subtle, and worthy of its own focused write-up. </p>

        </div>
    </article>
</body>
</html>